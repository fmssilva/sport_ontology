     all these jars... se tivessemos maven era mais facil?? 

    the queries are being writen as string. could we have some sort of ORM or something to have better syntax check...?

     * REASONING vs NON-REASONING SUMMARY:
     * 
     * NON-REASONING (our current tests):
     * ✓ Data integrity validation (SQL ↔ SPARQL consistency)  
     * ✓ OWA vs CWA assumption demonstration
     * ✓ Basic query correctness
     * 
     * REASONING (these new tests):
     * ✓ Class hierarchy inference (subClassOf)
     * ✓ Property domain/range inference  
     * ✓ Consistency checking (disjoint classes)
     * ✓ Inverse property derivation
     * 
     * LIMITATION: Ontop CLI has limited reasoning capabilities compared to 
     * full OWL reasoners like Pellet or HermiT. Some tests may not work as expected.



so how can we test with pellet or hermit? 

maybe i can add some examples in protege of ABox... test the hermit reasoner to see it work
then implement the code here in java to automate tests
and then when some test fails, we un that test in protege fr easier debug
does this sound a good strategy?
or how should we improve it?








also, maybe the TestRegistry class, is it still beeing used? if yes, to be in the config folder is it a good place to be? 
check the full strcture of the test folder and confirm where it should be... maybe inside the integration folder because it is some helper of the IntegrationTester or what's the best place? 

so again check all the code in the test folder and confirm it is everything cleaned and simple and well organized

and also check if there are unused files to delete inside the test folder 

make a complete check of our test folder and tell me if there are things we can make better and cleaner and simpler

and about the prrints, remoove all special chars you use in all files because in the terminaal it doesn't look good: 
? Running Basic Integrity SPARQL Tests...
Running SPARQL Tests (via Ontop CLI)...
    ? Executing SPARQL via Ontop: total_teams
  INT-01   | total_teams     | SPARQL | Expected:  7 | Actual:   7 | ? PASS (4196ms)
    ? Executing SPARQL via Ontop: total_players
  INT-02   | total_players   | SPARQL | Expected: 12 | Actual:  12 | ? PASS (4924ms)
    ? Executing SPARQL via Ontop: total_coaches
  INT-03   | total_coaches   | SPARQL | Expected:  7 | Actual:   7 | ? PASS (5024ms)
    └─ ??  Slow SPARQL query: 5024ms (threshold: 5000ms)
    ? Executing SPARQL via Ontop: total_contracts
  INT-04   | total_contracts | SPARQL | Expected: 10 | Actual:  10 | ? PASS (4599ms)
    ? Executing SPARQL via Ontop: team_distribution
  INT-05   | team_distribution | SPARQL | Expected:  5 | Actual:   5 | ? PASS (5203ms)
    └─ ??  Slow SPARQL query: 5203ms (threshold: 5000ms)

and then run again the test to confirm everything is ok




duvida??
The only remaining issue is INT-04 (active_contracts):

SQL: 9 contracts (correct - active contracts only)
SPARQL: 10 contracts (includes all contracts, not just active ones)
This is actually a perfect demonstration of OWA vs CWA!

SQL (CWA): Only counts explicitly is_active = TRUE contracts (9)
SPARQL (OWA): Counts all contracts mapped in the ontology (10), because the ontology doesn't have the isActive filter or treats missing active status as potentially true
Let me update the test to reflect this important distinction and make it a successful OWA vs CWA demonstration:







update the cross platform "bat" files to have the same info as currently our windows bat, because i think we changed it. 
and explain what is each type of file. bat, sh, no temrination?? 