review introduciton and conclusion




Now that we are passing all the tests, check if in the sports-ontology-report we should update any info related to that



Tests: 
remember i want sparql querries to access only the H2 DB data. sparql queries are NOT to include the ABox values. ABox is only for hermit.  
nevertheless, even if sparq is only to access the H2 DB, it is to perform the full stack query path through the ontop CLI, doing the reasoning procedures and to return the OWA and not the CWA. 


no report dizer que temos empty classes sem individuos, para manter a seed o mais pequeno possivel para raciocinar sobre os resultados esperaveis dos queries ....


see the project assignment requests. 
and see the deliverables i need to give
and see 
the code we have now implmented
and then confirm what task we have already done and to what level
is it good already for each area and task?
what should we do more?
keep in mind i want to go a bit beyong the requested to make this a very good prrojct 

---now see the full list of tests we add before enhancing our ontology 
and we are not gonna have time to implement all,
so from these list, now, give me a list of tests that we should implement
maybe the ones that are more interesting? and show reasoning? and OWA vs CWA? ??
see the assignment requirements and the topics i should write in the report
and see which tests we should implement
and so give me a list of those
we can continue having a good structure of tests domains, 
but lets implemnt just the best and most interesting tests in each one for now
so give me the list of tests to implement for me to confirm

and also a list of good seed to have for good testing of each query.
make the seed from both H2 DB and from ABox, to work for SQL (H2), SPARQ (H2 + ABox) and HERMIT (ABox)
in a way to make thee tests interesting and show interesting results
so i don't want a too big seed that becomes hard to check the results "by hand"
want a seed that is big enough to show interesting results but small enough we can run the tests by hand and confirm results 

then create a file "TOP_TESTS.md" and explain there the list of tests that would be nice for us to implement
and also the necessary H2 - SQL seed and ABox seed to have to run those tests


so now lets implement those, and also maybe delet the ones we have that are not that interesting 


about OWA vs CWA, print something to represent the DB tables with some examples there and for us to see the diference that for example an artist with the rule that has at least 1 song, in the DB, if the artist doesn't have any song, in SQL we won't return it, but in sparq we will becasue we assume the DB is not filled completly yet and so we assume true until negated. 
and also some concrete example of hierarchy differences
and what other differences? 

and to write the report maybe latex would be a good way to do it so you can help me? 
or even just md file? 
can we than export md file as pdf and it stay with good aspect of scientific paper? 



then add to the maven eexec:exec get deliverables... ?? 
the action of combining all the sparql and sql query files in the test folderin just one file *.q 
to add to protege-sparql and test there the queries also 
Criar ficheiro sports-tests.q com os queries sparql 
para depois Carregar no Protégé:

Tab Ontop SPARQL
File → Load queries
Selecione sports-tests.q
Execute uma a uma









     all these jars... se tivessemos maven era mais facil?? 

    the queries are being writen as string. could we have some sort of ORM or something to have better syntax check...?

     * REASONING vs NON-REASONING SUMMARY:
     * 
     * NON-REASONING (our current tests):
     * ✓ Data integrity validation (SQL ↔ SPARQL consistency)  
     * ✓ OWA vs CWA assumption demonstration
     * ✓ Basic query correctness
     * 
     * REASONING (these new tests):
     * ✓ Class hierarchy inference (subClassOf)
     * ✓ Property domain/range inference  
     * ✓ Consistency checking (disjoint classes)
     * ✓ Inverse property derivation
     * 
     * LIMITATION: Ontop CLI has limited reasoning capabilities compared to 
     * full OWL reasoners like Pellet or HermiT. Some tests may not work as expected.



so how can we test with pellet or hermit? 

maybe i can add some examples in protege of ABox... test the hermit reasoner to see it work
then implement the code here in java to automate tests
and then when some test fails, we un that test in protege fr easier debug
does this sound a good strategy?
or how should we improve it?








also, maybe the TestRegistry class, is it still beeing used? if yes, to be in the config folder is it a good place to be? 
check the full strcture of the test folder and confirm where it should be... maybe inside the integration folder because it is some helper of the IntegrationTester or what's the best place? 

so again check all the code in the test folder and confirm it is everything cleaned and simple and well organized

and also check if there are unused files to delete inside the test folder 

make a complete check of our test folder and tell me if there are things we can make better and cleaner and simpler

and about the prrints, remoove all special chars you use in all files because in the terminaal it doesn't look good: 
? Running Basic Integrity SPARQL Tests...
Running SPARQL Tests (via Ontop CLI)...
    ? Executing SPARQL via Ontop: total_teams
  INT-01   | total_teams     | SPARQL | Expected:  7 | Actual:   7 | ? PASS (4196ms)
    ? Executing SPARQL via Ontop: total_players
  INT-02   | total_players   | SPARQL | Expected: 12 | Actual:  12 | ? PASS (4924ms)
    ? Executing SPARQL via Ontop: total_coaches
  INT-03   | total_coaches   | SPARQL | Expected:  7 | Actual:   7 | ? PASS (5024ms)
    └─ ??  Slow SPARQL query: 5024ms (threshold: 5000ms)
    ? Executing SPARQL via Ontop: total_contracts
  INT-04   | total_contracts | SPARQL | Expected: 10 | Actual:  10 | ? PASS (4599ms)
    ? Executing SPARQL via Ontop: team_distribution
  INT-05   | team_distribution | SPARQL | Expected:  5 | Actual:   5 | ? PASS (5203ms)
    └─ ??  Slow SPARQL query: 5203ms (threshold: 5000ms)

and then run again the test to confirm everything is ok




duvida??
The only remaining issue is INT-04 (active_contracts):

SQL: 9 contracts (correct - active contracts only)
SPARQL: 10 contracts (includes all contracts, not just active ones)
This is actually a perfect demonstration of OWA vs CWA!

SQL (CWA): Only counts explicitly is_active = TRUE contracts (9)
SPARQL (OWA): Counts all contracts mapped in the ontology (10), because the ontology doesn't have the isActive filter or treats missing active status as potentially true
Let me update the test to reflect this important distinction and make it a successful OWA vs CWA demonstration:







update the cross platform "bat" files to have the same info as currently our windows bat, because i think we changed it. 
and explain what is each type of file. bat, sh, no temrination?? 


confirm that our mvn exec:exec@deliverables is working and capturing all the SPARQL files
and confirm that it correctly retrieves the ontology, mapping, and data seed currently in use
and confirm that it properly maintains the namespaces for H2 and SPARQL queries vs ABox for Hermit
and create a file with the important information needed to use that in Protégé.
for example, in Protégé: import the ontology, run the reasoner (e.g., Pellet or Hermit)
then add ontop... 
and in ontop mapping set the url to the H2 db... and give the path to use there?? and other things like the org.H2.Driver or somethign like this?? 
and then run the validate
and then the run ontop reasoner
and then the check consistency 
and then the queries, the *.q file
how to to run it? 
and now create some more easy to read file with the seed data we have for both sparq and in the ABoxs
so we can see easy the tables and individuals for example and then understand better the results in protege


»» and make the builder also add the bat to launch the H2 DB directly and to see that H2 in the browser... 
and clean the guides we have... 



add to mavn the command to run the ontop "check consistency" function. 
and also can we "run ontop resonner here locally too?, if yes so make the commnd do both: run ontop reasoner and then check for inconsistencies
and also the "validate mapping" "endpoint", before you used this command after copying files there to ontop folder??
cd tools ; .\ontop.bat validate -m mapping.ttl -t ontology.owl --db-driver=h2 --db-url="jdbc:h2:../sportsdb;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE" --db-user=sa
so it is easy for us to run and solve problmes 
